{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jordanchen/anaconda3/envs/mlbook/lib/python3.7/site-packages/lightgbm/__init__.py:46: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\nThis means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\nInstead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\nYou can install the OpenMP library by the following command: ``brew install libomp``.\n  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split, StratifiedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import log_loss, roc_auc_score, auc, roc_curve, accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from xgboost import XGBClassifier               \n",
    "from lightgbm import LGBMClassifier\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from mlxtend.regressor import StackingRegressor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1160742, 7)\n(306313, 6)\n(1467055, 7)\n"
     ]
    }
   ],
   "source": [
    "DATA_ROOT = \"ml100marathon-02-01/\"\n",
    "dftrain = pd.read_csv(os.path.join(DATA_ROOT,'train_offline.csv'))\n",
    "dftest = pd.read_csv(os.path.join(DATA_ROOT,'test_offline.csv'))\n",
    "dftest = dftest[~dftest.Coupon_id.isna()]\n",
    "dftest.reset_index(drop=True, inplace=True)\n",
    "dftotal = pd.concat([dftrain, dftest])\n",
    "print(dftrain.shape)\n",
    "print(dftest.shape)\n",
    "print(dftotal.shape)\n",
    "# print(dfoff.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of missing data in training dataset: \n Missing values in Coupon_id : 413773\n Missing values in Discount_rate : 413773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Missing values in Distance : 69826\n Missing values in Date_received : 413773\n Missing values in Date : 704033\n\n\nCount of missing data in testing dataset: \n Missing values in Distance : 36177\n\n\nCount of missing data in total dataset: \n Missing values in Coupon_id : 413773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Missing values in Date : 1010346\n Missing values in Date_received : 413773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Missing values in Discount_rate : 413773\n Missing values in Distance : 106003\n"
     ]
    }
   ],
   "source": [
    "def CheckMissingVals(data):\n",
    "    for col in data.columns:\n",
    "        if np.sum(data[col].isnull()) != 0:\n",
    "            print(f' Missing values in {col} : {np.sum(data[col].isnull())}')\n",
    "\n",
    "print(\"Count of missing data in training dataset: \")\n",
    "CheckMissingVals(dftrain)\n",
    "print('\\n')\n",
    "print(\"Count of missing data in testing dataset: \")\n",
    "CheckMissingVals(dftest)\n",
    "print('\\n')\n",
    "print(\"Count of missing data in total dataset: \")\n",
    "CheckMissingVals(dftotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Coupon_id        Date  Date_received Discount_rate  Distance  Merchant_id  \\\n0         NaN  20160217.0            NaN           NaN       0.0         2632   \n1      8591.0         NaN     20160217.0          20:1       0.0         2632   \n2      1078.0         NaN     20160319.0          20:1       0.0         2632   \n3      7610.0         NaN     20160429.0        200:20       0.0         3381   \n4     11951.0         NaN     20160129.0        200:20       1.0         3381   \n5      9776.0         NaN     20160129.0          10:5       2.0         3381   \n6     12034.0         NaN     20160207.0        100:10       NaN         2099   \n7      5054.0         NaN     20160421.0        200:30      10.0         1569   \n8      7802.0         NaN     20160130.0        200:20      10.0         4833   \n9      7610.0         NaN     20160412.0        200:20       2.0         3381   \n10        NaN  20160327.0            NaN           NaN       0.0         8390   \n11     7531.0         NaN     20160327.0          20:5       0.0         8390   \n12    13490.0         NaN     20160127.0          30:5       2.0         1041   \n13        NaN  20160115.0            NaN           NaN      10.0         7884   \n14     6704.0         NaN     20160215.0          20:1      10.0         7884   \n15        NaN  20160114.0            NaN           NaN       2.0         1041   \n16    11197.0         NaN     20160114.0          30:5       2.0         1041   \n17      111.0         NaN     20160207.0          30:5       1.0         5341   \n18     7531.0  20160329.0     20160321.0          20:5       0.0         8390   \n19      111.0         NaN     20160207.0          30:5       1.0         5341   \n\n    User_id  DistanceFilling_UM  DistanceFilling_U  DistanceFilling_M  \n0   1439408                 0.0           0.166667           1.456522  \n1   1439408                 0.0           0.166667           1.456522  \n2   1439408                 0.0           0.166667           1.456522  \n3   1832624                 0.0           0.000000           2.773611  \n4   2029232                 1.0           0.333333           2.773611  \n5   2223968                 2.0           2.000000           2.773611  \n6     73611                 NaN                NaN           2.010832  \n7    163606                10.0          10.000000           6.188216  \n8   3273056                10.0          10.000000           7.042252  \n9     94107                 2.0           2.000000           2.773611  \n10   253750                 0.0           0.000000           0.520722  \n11   253750                 0.0           0.000000           0.520722  \n12   376492                 2.0           2.000000           3.049795  \n13  1964720                10.0          10.000000           2.412805  \n14  1964720                10.0          10.000000           2.412805  \n15  1113008                 2.0           0.666667           3.049795  \n16  1113008                 2.0           0.666667           3.049795  \n17  2881376                 1.0           0.500000           0.453050  \n18  2881376                 0.0           0.500000           0.520722  \n19  2881376                 1.0           0.500000           0.453050  \n"
     ]
    }
   ],
   "source": [
    "DistanceFilling_UM = dftotal.groupby(['User_id','Merchant_id'])['Distance'].mean().reset_index()\n",
    "DistanceFilling_UM.columns = ['User_id','Merchant_id','DistanceFilling_UM']\n",
    "DistanceFilling_U = dftotal.groupby(['User_id'])['Distance'].mean().reset_index()\n",
    "DistanceFilling_U.columns = ['User_id','DistanceFilling_U']\n",
    "DistanceFilling_M = dftotal.groupby(['Merchant_id'])['Distance'].mean().reset_index()\n",
    "DistanceFilling_M.columns = ['Merchant_id','DistanceFilling_M']\n",
    "\n",
    "dftotal = pd.merge(dftotal,DistanceFilling_UM,on = ['User_id','Merchant_id'], how = 'left')\n",
    "dftotal = pd.merge(dftotal,DistanceFilling_U,on = ['User_id'], how = 'left')\n",
    "dftotal = pd.merge(dftotal,DistanceFilling_M,on = ['Merchant_id'], how = 'left')\n",
    "print(dftotal.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DistanceMissingFill(data):\n",
    "    if np.isnan(data['Distance']):\n",
    "        if not np.isnan(data['DistanceFilling_UM']):\n",
    "            return int(data['DistanceFilling_UM'])\n",
    "        elif not np.isnan(data['DistanceFilling_U']):\n",
    "            return int(data['DistanceFilling_U'])\n",
    "        elif not  np.isnan(data['DistanceFilling_M']):\n",
    "            return int(data['DistanceFilling_M'])\n",
    "    return data['Distance']\n",
    "\n",
    "dftotal['Distance'] = dftotal.apply(DistanceMissingFill, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0    1016978\n-1     413773\n 1      36304\nName: label, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Creat target label \n",
    "\"\"\"\n",
    "According to the definition, \n",
    "1) buy with coupon within (include) 15 days ==> 1\n",
    "2) buy with coupon but out of 15 days ==> 0\n",
    "3) buy without coupon ==> -1 (we don't care)\n",
    "\"\"\"\n",
    "def label(row):\n",
    "    if np.isnan(row['Date_received']):\n",
    "        return -1\n",
    "    if not np.isnan(row['Date']):\n",
    "        td = pd.to_datetime(row['Date'], format='%Y%m%d') -  pd.to_datetime(row['Date_received'], format='%Y%m%d')\n",
    "        if td <= pd.Timedelta(15, 'D'):\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "dftotal[\"label\"] = dftotal.apply(label, axis=1)\n",
    "dftotal[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate features - weekday acquired coupon\n",
    "def getWeekday(row):\n",
    "    if (np.isnan(row)) or (row==-1):\n",
    "        return row\n",
    "    else:\n",
    "        return pd.to_datetime(row, format = \"%Y%m%d\").dayofweek+1 # add one to make it from 0~6 -> 1~7\n",
    "\n",
    "# dfoff['weekday'] = dfoff['Date_received'].apply(getWeekday)\n",
    "# dftest['weekday'] = dftest['Date_received'].apply(getWeekday)\n",
    "# \n",
    "# # weekday_type (weekend = 1)\n",
    "# dfoff['weekday_type'] = dfoff['weekday'].astype('str').apply(lambda x : 1 if x in [6,7] else 0 ) # apply to trainset\n",
    "# dftest['weekday_type'] = dftest['weekday'].astype('str').apply(lambda x : 1 if x in [6,7] else 0 ) # apply to testset\n",
    "\n",
    "dftotal['weekday'] = dftotal['Date_received'].apply(getWeekday)\n",
    "dftotal['weekday_type'] = dftotal['weekday'].astype('str').apply(lambda x : 1 if x in [6,7] else 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['weekday_1', 'weekday_2', 'weekday_3', 'weekday_4', 'weekday_5', 'weekday_6', 'weekday_7']\n"
     ]
    }
   ],
   "source": [
    "weekdaycols = ['weekday_' + str(i) for i in range(1,8)]\n",
    "print(weekdaycols)\n",
    "\n",
    "# tmpdf = pd.get_dummies(dfoff['weekday'].replace(-1, np.nan))5\n",
    "# tmpdf.columns = weekdaycols\n",
    "# dfoff[weekdaycols] = tmpdf\n",
    "# \n",
    "# tmpdf = pd.get_dummies(dftest['weekday'].replace(-1, np.nan))\n",
    "# tmpdf.columns = weekdaycols\n",
    "# dftest[weekdaycols] = tmpdf\n",
    "\n",
    "tmpdf = pd.get_dummies(dftotal['weekday'].replace(-1, np.nan))\n",
    "tmpdf.columns = weekdaycols\n",
    "dftotal[weekdaycols] = tmpdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate features - coupon discount and distance\n",
    "def getDiscountType(row):\n",
    "    if row == 'null':\n",
    "        return 'null'\n",
    "    elif ':' in row:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def convertRate(row):\n",
    "    \"\"\"Convert discount to rate\"\"\"\n",
    "    if row == 'null':\n",
    "        return 1.0\n",
    "    elif ':' in row:\n",
    "        rows = row.split(':')\n",
    "        return 1.0 - float(rows[1])/float(rows[0])\n",
    "    else:\n",
    "        return float(row)\n",
    "\n",
    "def getDiscountMan(row):\n",
    "    if ':' in row:\n",
    "        rows = row.split(':')\n",
    "        return int(rows[0])\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def getDiscountJian(row):\n",
    "    if ':' in row:\n",
    "        rows = row.split(':')\n",
    "        return int(rows[1])\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def processData(df):\n",
    "    \n",
    "    # convert discunt_rate\n",
    "    df['discount_rate'] = df['Discount_rate'].astype('str').apply(convertRate)\n",
    "    df['discount_man'] = df['Discount_rate'].astype('str').apply(getDiscountMan)\n",
    "    df['discount_jian'] = df['Discount_rate'].astype('str').apply(getDiscountJian)\n",
    "    df['discount_type'] = df['Discount_rate'].astype('str').apply(getDiscountType)\n",
    "    \n",
    "    # convert distance\n",
    "    df.loc[df.Distance.isna(), \"Distance\"] = 99\n",
    "    return df\n",
    "\n",
    "# dfoff = processData(dfoff)\n",
    "# dftest = processData(dftest)\n",
    "dftotal = processData(dftotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of missing data in training dataset: \n Missing values in Coupon_id : 413773\n Missing values in Date : 704033\n Missing values in Date_received : 413773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Missing values in Discount_rate : 413773\n Missing values in weekday : 413773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Missing values in discount_rate : 413773\n\n\nCount of missing data in testing dataset: \n\n\nShape of training set - (1160742, 21)\nShape of testing set - (306313, 20)\n"
     ]
    }
   ],
   "source": [
    "# dftotal = dftotal.drop(['DistanceFilling_UM','DistanceFilling_U','DistanceFilling_M'], axis = 1)\n",
    "dftrain = dftotal[:len(dftrain)]\n",
    "dftest = dftotal[len(dftrain):].drop(['Date'], axis = 1)\n",
    "\n",
    "print(\"Count of missing data in training dataset: \")\n",
    "CheckMissingVals(dftrain)\n",
    "print('\\n')\n",
    "print(\"Count of missing data in testing dataset: \")\n",
    "CheckMissingVals(dftest)\n",
    "\n",
    "print('\\n')\n",
    "print(\"Shape of training set -\", dftrain.shape)\n",
    "print(\"Shape of testing set -\", dftest.shape)\n",
    "# print(dftrain.shape)\n",
    "# print(dftest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Naive model\n",
    "def split_train_valid(row, date_cut=\"20160416\"):\n",
    "    is_train = True if pd.to_datetime(row, format=\"%Y%m%d\") < pd.to_datetime(date_cut, format=\"%Y%m%d\") else False\n",
    "    return is_train\n",
    "    \n",
    "df = dftrain[dftrain['label'] != -1].copy()\n",
    "df[\"is_train\"] = df[\"Date_received\"].apply(split_train_valid)\n",
    "train = df[df[\"is_train\"]]\n",
    "valid = df[~df[\"is_train\"]]\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "valid.reset_index(drop=True, inplace=True)\n",
    "print(\"Train size: {}, #positive: {}\".format(len(train), train[\"label\"].sum()))\n",
    "print(\"Valid size: {}, #positive: {}\".format(len(valid), valid[\"label\"].sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 ['discount_rate', 'discount_type', 'discount_man', 'discount_jian', 'Distance', 'weekday', 'weekday_type', 'weekday_1', 'weekday_2', 'weekday_3', 'weekday_4', 'weekday_5', 'weekday_6', 'weekday_7']\n         Coupon_id  Date_received Discount_rate  Distance  Merchant_id  \\\n1160742    11002.0     20160528.0        150:20       1.0         4663   \n1160743     8591.0     20160613.0          20:1       0.0         2632   \n1160744     8591.0     20160516.0          20:1       0.0         2632   \n1160745     1532.0     20160530.0          30:5       0.0          450   \n1160746    12737.0     20160519.0          20:1       0.0         6459   \n\n         User_id  label  weekday  weekday_type  weekday_1  weekday_2  \\\n1160742  1439408      0      6.0             0          0          0   \n1160743  1439408      0      1.0             0          1          0   \n1160744  1439408      0      1.0             0          1          0   \n1160745  2029232      0      1.0             0          1          0   \n1160746  2029232      0      4.0             0          0          0   \n\n         weekday_3  weekday_4  weekday_5  weekday_6  weekday_7  discount_rate  \\\n1160742          0          0          0          1          0       0.866667   \n1160743          0          0          0          0          0       0.950000   \n1160744          0          0          0          0          0       0.950000   \n1160745          0          0          0          0          0       0.833333   \n1160746          0          1          0          0          0       0.950000   \n\n         discount_man  discount_jian  discount_type  \n1160742           150             20              1  \n1160743            20              1              1  \n1160744            20              1              1  \n1160745            30              5              1  \n1160746            20              1              1  \n"
     ]
    }
   ],
   "source": [
    "original_feature = ['discount_rate',\n",
    "                    'discount_type',\n",
    "                    'discount_man', \n",
    "                    'discount_jian',\n",
    "                    'Distance', \n",
    "                    'weekday', \n",
    "                    'weekday_type'] + weekdaycols\n",
    "print(len(original_feature),original_feature)\n",
    "print(dftest.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['discount_rate', 'discount_type', 'discount_man', 'discount_jian', 'Distance', 'weekday', 'weekday_type', 'weekday_1', 'weekday_2', 'weekday_3', 'weekday_4', 'weekday_5', 'weekday_6', 'weekday_7']\n"
     ]
    }
   ],
   "source": [
    "# predictors = original_feature\n",
    "# print(predictors)\n",
    "# \n",
    "# def check_model(data, predictors):\n",
    "#     \n",
    "#     # classifier = lambda: SGDClassifier(\n",
    "#     #     loss='log', \n",
    "#     #     penalty='elasticnet', \n",
    "#     #     fit_intercept=True, \n",
    "#     #     max_iter=100, \n",
    "#     #     shuffle=True, \n",
    "#     #     n_jobs=1,\n",
    "#     #     class_weight=None)\n",
    "#     \n",
    "#     classifier = lambda: RandomForestClassifier(\n",
    "#         n_jobs=1,\n",
    "#         class_weight=None)\n",
    "# \n",
    "#     model = Pipeline(steps=[\n",
    "#         ('ss', StandardScaler()),\n",
    "#         ('en', classifier())\n",
    "#     ])\n",
    "# \n",
    "#     # parameters = [{\n",
    "#     #     'en__alpha': [ 0.001, 0.01, 0.1],\n",
    "#     #     'en__l1_ratio': [ 0.001, 0.01, 0.1]\n",
    "#     # },\n",
    "#     # {\n",
    "#     #     'bootstrap': [False],\n",
    "#     #     'en__alpha': [ 0.001, 0.01, 0.1],\n",
    "#     #     'en__l1_ratio': [ 0.001, 0.01, 0.1]\n",
    "#     # }]\n",
    "#     \n",
    "#     parameters = [\n",
    "#     {\n",
    "#         'bootstrap': [False]\n",
    "#     }]\n",
    "# \n",
    "#     folder = StratifiedKFold(n_splits=3, shuffle=True)\n",
    "#     \n",
    "#     grid_search = GridSearchCV(\n",
    "#         model, \n",
    "#         parameters, \n",
    "#         cv=folder, \n",
    "#         n_jobs=-1, \n",
    "#         verbose=1)\n",
    "#     grid_search = grid_search.fit(data[predictors], \n",
    "#                                   data['label'])\n",
    "#     \n",
    "#     return grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid parameter bootstrap for estimator Pipeline(memory=None,\n     steps=[('ss', StandardScaler(copy=True, with_mean=True, with_std=True)), ('en', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=None, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n          ...n_jobs=1,\n            oob_score=False, random_state=None, verbose=0,\n            warm_start=False))]). Check the list of available parameters with `estimator.get_params().keys()`.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/Users/jordanchen/anaconda3/envs/mlbook/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py\", line 418, in _process_worker\n    r = call_item()\n  File \"/Users/jordanchen/anaconda3/envs/mlbook/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py\", line 272, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"/Users/jordanchen/anaconda3/envs/mlbook/lib/python3.7/site-packages/sklearn/externals/joblib/_parallel_backends.py\", line 567, in __call__\n    return self.func(*args, **kwargs)\n  File \"/Users/jordanchen/anaconda3/envs/mlbook/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\", line 225, in __call__\n    for func, args, kwargs in self.items]\n  File \"/Users/jordanchen/anaconda3/envs/mlbook/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\", line 225, in <listcomp>\n    for func, args, kwargs in self.items]\n  File \"/Users/jordanchen/anaconda3/envs/mlbook/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\", line 514, in _fit_and_score\n    estimator.set_params(**parameters)\n  File \"/Users/jordanchen/anaconda3/envs/mlbook/lib/python3.7/site-packages/sklearn/pipeline.py\", line 147, in set_params\n    self._set_params('steps', **kwargs)\n  File \"/Users/jordanchen/anaconda3/envs/mlbook/lib/python3.7/site-packages/sklearn/utils/metaestimators.py\", line 52, in _set_params\n    super(_BaseComposition, self).set_params(**params)\n  File \"/Users/jordanchen/anaconda3/envs/mlbook/lib/python3.7/site-packages/sklearn/base.py\", line 215, in set_params\n    (key, self))\nValueError: Invalid parameter bootstrap for estimator Pipeline(memory=None,\n     steps=[('ss', StandardScaler(copy=True, with_mean=True, with_std=True)), ('en', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=None, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n          ...n_jobs=1,\n            oob_score=False, random_state=None, verbose=0,\n            warm_start=False))]). Check the list of available parameters with `estimator.get_params().keys()`.\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-d89d7bcd07fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-30-72316de94700>\u001b[0m in \u001b[0;36mcheck_model\u001b[0;34m(data, predictors)\u001b[0m\n\u001b[1;32m     46\u001b[0m         verbose=1)\n\u001b[1;32m     47\u001b[0m     grid_search = grid_search.fit(data[predictors], \n\u001b[0;32m---> 48\u001b[0;31m                                   data['label'])\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlbook/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlbook/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlbook/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    709\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 711\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m                 \u001b[0mall_candidate_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlbook/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlbook/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlbook/lib/python3.7/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlbook/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    430\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mlbook/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid parameter bootstrap for estimator Pipeline(memory=None,\n     steps=[('ss', StandardScaler(copy=True, with_mean=True, with_std=True)), ('en', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=None, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n          ...n_jobs=1,\n            oob_score=False, random_state=None, verbose=0,\n            warm_start=False))]). Check the list of available parameters with `estimator.get_params().keys()`."
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# model = check_model(train, predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['discount_rate', 'discount_type', 'discount_man', 'discount_jian', 'Distance', 'weekday', 'weekday_type', 'weekday_1', 'weekday_2', 'weekday_3', 'weekday_4', 'weekday_5', 'weekday_6', 'weekday_7']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=None, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n            oob_score=False, random_state=None, verbose=0,\n            warm_start=False),\n       fit_params=None, iid='warn', n_jobs=None,\n       param_grid=[{'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]}, {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]}],\n       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n       scoring='neg_mean_squared_error', verbose=0)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # RandomForestClassifier\n",
    "# predictors = original_feature\n",
    "# print(predictors)\n",
    "# param_grid = [\n",
    "#         {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n",
    "#         {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
    "# ]\n",
    "# forest_reg = RandomForestClassifier()\n",
    "# model = GridSearchCV(forest_reg, param_grid, cv=5,\n",
    "#                             scoring='neg_mean_squared_error',\n",
    "#                             return_train_score=True)\n",
    "# model.fit(train[predictors], train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['discount_rate', 'discount_type', 'discount_man', 'discount_jian', 'Distance', 'weekday', 'weekday_type', 'weekday_1', 'weekday_2', 'weekday_3', 'weekday_4', 'weekday_5', 'weekday_6', 'weekday_7']\n"
     ]
    }
   ],
   "source": [
    "# XGBClassifier\n",
    "predictors = original_feature\n",
    "print(predictors)\n",
    "param_grid = [\n",
    "        {'n_estimators': [3, 10, 30], \n",
    "         'max_features': [8, 10, 12, 14], \n",
    "         'learning_rate': [0.01, 0.02, 0.03], \n",
    "         'max_depth': [4, 5, 6]}\n",
    "]\n",
    "xgb_model = XGBClassifier()\n",
    "model = GridSearchCV(xgb_model, \n",
    "                     param_grid, \n",
    "                     cv=5,\n",
    "                     scoring='roc_auc',\n",
    "                     return_train_score=True)\n",
    "model.fit(train[predictors], train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid_pred = model.predict_proba(valid[predictors])\n",
    "valid1 = valid.copy()\n",
    "valid1['pred_prob'] = y_valid_pred[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation AUC: 0.771, Accuracy: 0.951\n"
     ]
    }
   ],
   "source": [
    "auc_score = roc_auc_score(y_true=valid.label, y_score=y_valid_pred[:,1])\n",
    "acc = accuracy_score(y_true=valid.label, y_pred=y_valid_pred.argmax(axis=1))\n",
    "print(\"Validation AUC: {:.3f}, Accuracy: {:.3f}\".format(auc_score, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(306313, 20)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(306313, 15)\n"
     ]
    }
   ],
   "source": [
    "targetset = dftest.copy()\n",
    "print(targetset.shape)\n",
    "targetset = targetset[~targetset.Coupon_id.isna()]\n",
    "targetset.reset_index(drop=True, inplace=True)\n",
    "testset = targetset[predictors].copy()\n",
    "\n",
    "y_test_pred = model.predict_proba(testset[predictors])\n",
    "test1 = testset.copy()\n",
    "test1['pred_prob'] = y_test_pred[:, 1]\n",
    "print(test1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(306313, 4)\n"
     ]
    }
   ],
   "source": [
    "output = pd.concat((targetset[[\"User_id\", \"Coupon_id\", \"Date_received\"]], test1[\"pred_prob\"]), axis=1)\n",
    "print(output.shape)\n",
    "\n",
    "output.loc[:, \"User_id\"] = output[\"User_id\"].apply(lambda x:str(int(x)))\n",
    "output.loc[:, \"Coupon_id\"] = output[\"Coupon_id\"].apply(lambda x:str(int(x)))\n",
    "output.loc[:, \"Date_received\"] = output[\"Date_received\"].apply(lambda x:str(int(x)))\n",
    "output[\"uid\"] = output[[\"User_id\", \"Coupon_id\", \"Date_received\"]].apply(lambda x: '_'.join(x.values), axis=1)\n",
    "output.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     uid     label\n0  1000020_2705_20160519  0.216529\n1  1000020_8192_20160513  0.204398\n2  1000065_1455_20160527  0.098958\n3  1000085_8067_20160513  0.136849\n4  1000086_2418_20160613  0.157008\n"
     ]
    }
   ],
   "source": [
    "### NOTE: YOUR SUBMITION FILE SHOULD HAVE COLUMN NAME: uid, label\n",
    "out = output.groupby(\"uid\", as_index=False).mean()\n",
    "out = out[[\"uid\", \"pred_prob\"]]\n",
    "out.columns = [\"uid\", \"label\"]\n",
    "out.to_csv(\"homework/midterm_submission.csv\", header=[\"uid\", \"label\"], index=False) # submission format\n",
    "print(out.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
