{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md\ndata\nhomework\nkaggle\n\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \".\"]).decode(\"utf8\"))\n",
    "train = pd.read_csv('./kaggle/data-science-london-scikit-learn/train.csv', header=None)\n",
    "trainLabel = pd.read_csv('./kaggle/data-science-london-scikit-learn/trainLabels.csv', header=None)\n",
    "test = pd.read_csv('./kaggle/data-science-london-scikit-learn/test.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('train shape:', (1000, 40))\n('test shape:', (9000, 40))\n('trainLabel shape:', (1000, 1))\n         0         1         2         3         4         5         6   \\\n0  0.299403 -1.226624  1.498425 -1.176150  5.289853  0.208297  2.404498   \n1 -1.174176  0.332157  0.949919 -1.285328  2.199061 -0.151268 -0.427039   \n2  1.192222 -0.414371  0.067054 -2.233568  3.658881  0.089007  0.203439   \n3  1.573270 -0.580318 -0.866332 -0.603812  3.125716  0.870321 -0.161992   \n4 -0.613071 -0.644204  1.112558 -0.032397  3.490142 -0.011935  1.443521   \n\n         7         8         9   ...        30        31        32        33  \\\n0  1.594506 -0.051608  0.663234  ... -0.850465 -0.622990 -1.833057  0.293024   \n1  2.619246 -0.765884 -0.093780  ... -0.819750  0.012037  2.038836  0.468579   \n2 -4.219054 -1.184919 -1.240310  ... -0.604501  0.750054 -3.360521  0.856988   \n3  4.499666  1.038741 -1.092716  ...  1.022959  1.275598 -3.480110 -1.065252   \n4 -4.290282 -1.761308  0.807652  ...  0.513906 -1.803473  0.518579 -0.205029   \n\n         34        35        36        37        38        39  \n0  3.552681  0.717611  3.305972 -2.715559 -2.682409  0.101050  \n1 -0.517657  0.422326  0.803699  1.213219  1.382932 -1.817761  \n2 -2.751451 -1.582735  1.672246  0.656438 -0.932473  2.987436  \n3  2.153133  1.563539  2.767117  0.215748  0.619645  1.883397  \n4 -4.744566 -1.520015  1.830651  0.870772 -1.894609  0.408332  \n\n[5 rows x 40 columns]\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1000 entries, 0 to 999\nData columns (total 40 columns):\n0     1000 non-null float64\n1     1000 non-null float64\n2     1000 non-null float64\n3     1000 non-null float64\n4     1000 non-null float64\n5     1000 non-null float64\n6     1000 non-null float64\n7     1000 non-null float64\n8     1000 non-null float64\n9     1000 non-null float64\n10    1000 non-null float64\n11    1000 non-null float64\n12    1000 non-null float64\n13    1000 non-null float64\n14    1000 non-null float64\n15    1000 non-null float64\n16    1000 non-null float64\n17    1000 non-null float64\n18    1000 non-null float64\n19    1000 non-null float64\n20    1000 non-null float64\n21    1000 non-null float64\n22    1000 non-null float64\n23    1000 non-null float64\n24    1000 non-null float64\n25    1000 non-null float64\n26    1000 non-null float64\n27    1000 non-null float64\n28    1000 non-null float64\n29    1000 non-null float64\n30    1000 non-null float64\n31    1000 non-null float64\n32    1000 non-null float64\n33    1000 non-null float64\n34    1000 non-null float64\n35    1000 non-null float64\n36    1000 non-null float64\n37    1000 non-null float64\n38    1000 non-null float64\n39    1000 non-null float64\ndtypes: float64(40)\nmemory usage: 312.6 KB\n"
     ]
    }
   ],
   "source": [
    "print('train shape:', train.shape)\n",
    "print('test shape:', test.shape)\n",
    "print('trainLabel shape:', trainLabel.shape)\n",
    "print(train.head())\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "\n",
    "X, y = train, np.ravel(trainLabel)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Best Accuracy without feature scaling:', 0.9109999999999999)\nKNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n           metric_params=None, n_jobs=None, n_neighbors=3, p=2,\n           weights='uniform')\n"
     ]
    }
   ],
   "source": [
    "neig = np.arange(1, 25)\n",
    "kfold = 10\n",
    "train_accuracy = []\n",
    "val_accuracy = []\n",
    "bestKnn = None\n",
    "bestAcc = 0.0\n",
    "# Loop over different values of k\n",
    "for i, k in enumerate(neig):\n",
    "    # k from 1 to 25(exclude)\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    # Fit with knn\n",
    "    knn.fit(X_train,y_train)\n",
    "    #train accuracy\n",
    "    train_accuracy.append(knn.score(X_train, y_train))\n",
    "    # test accuracy\n",
    "    val_accuracy.append(np.mean(cross_val_score(knn, X, y, cv=kfold)))\n",
    "    if np.mean(cross_val_score(knn, X, y, cv=kfold)) > bestAcc:\n",
    "        bestAcc = np.mean(cross_val_score(knn, X, y, cv=10))\n",
    "        bestKnn = knn\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=[13,8])\n",
    "plt.plot(neig, val_accuracy, label = 'Validation Accuracy')\n",
    "plt.plot(neig, train_accuracy, label = 'Training Accuracy')\n",
    "plt.legend()\n",
    "plt.title('k value VS Accuracy')\n",
    "plt.xlabel('Number of Neighbors')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks(neig)\n",
    "plt.show()\n",
    "\n",
    "print('Best Accuracy without feature scaling:', bestAcc)\n",
    "print(bestKnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9000, 1)\n"
     ]
    }
   ],
   "source": [
    "# predict test\n",
    "test_fill = np.nan_to_num(test)\n",
    "submission = pd.DataFrame(bestKnn.predict(test_fill))\n",
    "print(submission.shape)\n",
    "submission.columns = ['Solution']\n",
    "submission['Id'] = np.arange(1,submission.shape[0]+1)\n",
    "submission = submission[['Id', 'Solution']]\n",
    "# print(submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission_no_normalization.csv\nsubmission_with_scaling.csv\ntest.csv\ntrain.csv\ntrainLabels.csv\n\n"
     ]
    }
   ],
   "source": [
    "submission.to_csv('./kaggle/data-science-london-scikit-learn/submission_no_normalization.csv', index=False)\n",
    "print(check_output([\"ls\", \"./kaggle/data-science-london-scikit-learn\"]).decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer\n",
    "\n",
    "std = StandardScaler()\n",
    "X_std = std.fit_transform(X)\n",
    "mms = MinMaxScaler()\n",
    "X_mms = mms.fit_transform(X)\n",
    "norm = Normalizer()\n",
    "X_norm = norm.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Best Accuracy with feature scaling:', 0.9019999999999999)\n('Best kNN classifier:', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n           weights='uniform'))\n('Best scaling:', 'norm')\n"
     ]
    }
   ],
   "source": [
    "# Model complexity\n",
    "neig = np.arange(1, 30)\n",
    "kfold = 10\n",
    "val_accuracy = {'std':[], 'mms':[], 'norm':[]}\n",
    "bestKnn = None\n",
    "bestAcc = 0.0\n",
    "bestScaling = None\n",
    "# Loop over different values of k\n",
    "for i, k in enumerate(neig):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    # validation accuracy\n",
    "    s1 = np.mean(cross_val_score(knn, X_std, y, cv=kfold))\n",
    "    val_accuracy['std'].append(s1)\n",
    "    s2 = np.mean(cross_val_score(knn, X_mms, y, cv=kfold))\n",
    "    val_accuracy['mms'].append(s2)\n",
    "    s3 = np.mean(cross_val_score(knn, X_norm, y, cv=kfold))\n",
    "    val_accuracy['norm'].append(s3)\n",
    "    if s1 > bestAcc:\n",
    "        bestAcc = s1\n",
    "        bestKnn = knn\n",
    "        bestScaling = 'std'\n",
    "    elif s2 > bestAcc:\n",
    "        bestAcc = s2\n",
    "        bestKnn = knn\n",
    "        bestScaling = 'mms'\n",
    "    elif s3 > bestAcc:\n",
    "        bestAcc = s3\n",
    "        bestKnn = knn\n",
    "        bestScaling = 'norm'\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=[13,8])\n",
    "plt.plot(neig, val_accuracy['std'], label = 'CV Accuracy with std')\n",
    "plt.plot(neig, val_accuracy['mms'], label = 'CV Accuracy with mms')\n",
    "plt.plot(neig, val_accuracy['norm'], label = 'CV Accuracy with norm')\n",
    "plt.legend()\n",
    "plt.title('k value VS Accuracy')\n",
    "plt.xlabel('Number of Neighbors')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks(neig)\n",
    "plt.show()\n",
    "\n",
    "print('Best Accuracy with feature scaling:', bestAcc)\n",
    "print('Best kNN classifier:', bestKnn)\n",
    "print('Best scaling:', bestScaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9000, 1)\n"
     ]
    }
   ],
   "source": [
    "# predict on test\n",
    "bestKnn.fit(X_norm, y)\n",
    "submission = pd.DataFrame(bestKnn.predict(norm.transform(test_fill)))\n",
    "print(submission.shape)\n",
    "submission.columns = ['Solution']\n",
    "submission['Id'] = np.arange(1,submission.shape[0]+1)\n",
    "submission = submission[['Id', 'Solution']]\n",
    "# submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission_no_normalization.csv\nsubmission_with_scaling.csv\ntest.csv\ntrain.csv\ntrainLabels.csv\n\n"
     ]
    }
   ],
   "source": [
    "submission.to_csv('./kaggle/data-science-london-scikit-learn/submission_with_scaling.csv', index=False)\n",
    "print(check_output([\"ls\", \"./kaggle/data-science-london-scikit-learn\"]).decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a1934a750>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#correlation map\n",
    "f,ax = plt.subplots(figsize=(18, 18))\n",
    "sns.heatmap(pd.DataFrame(X_std).corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jordanchen/anaconda3/envs/ml_book/lib/python2.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Accuracy is: ', 0.8366666666666667)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a1934a750>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score,confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# split data train 70 % and val 30 %\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_std, y, test_size=0.3, random_state=42)\n",
    "\n",
    "#random forest classifier with n_estimators=10 (default)\n",
    "clf_rf = RandomForestClassifier(random_state=43)      \n",
    "clr_rf = clf_rf.fit(X_train,y_train)\n",
    "\n",
    "ac = accuracy_score(y_val,clf_rf.predict(X_val))\n",
    "print('Accuracy is: ',ac)\n",
    "cm = confusion_matrix(y_val,clf_rf.predict(X_val))\n",
    "sns.heatmap(cm,annot=True,fmt=\"d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Best Accuracy with feature scaling and RFECV:', 0.827)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "kfold = 10\n",
    "bestSVC = None\n",
    "bestAcc = 0.0\n",
    "val_accuracy = []\n",
    "cv_range = np.arange(5, 11)\n",
    "n_feature = []\n",
    "for cv in cv_range:\n",
    "    # Create the RFE object and compute a cross-validated score.\n",
    "    svc = SVC(kernel=\"linear\")\n",
    "    # The \"accuracy\" scoring is proportional to the number of correct\n",
    "    # classifications\n",
    "    rfecv = RFECV(estimator=svc, step=1, cv=cv, scoring='accuracy')\n",
    "    rfecv.fit(X_std, y)\n",
    "\n",
    "    # print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
    "    # print('Best features :', pd.DataFrame(X_train).columns[rfecv.support_])\n",
    "\n",
    "    # Model complexity\n",
    "    val_accuracy += [np.mean(cross_val_score(svc, X_std[:, rfecv.support_], y, cv=kfold))]\n",
    "    n_feature.append(rfecv.n_features_)\n",
    "    if val_accuracy[-1] > bestAcc:\n",
    "        bestAcc = val_accuracy[-1]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=[13,8])\n",
    "plt.plot(cv_range, val_accuracy, label = 'CV Accuracy')\n",
    "for i in range(len(cv_range)):\n",
    "    plt.annotate(str(n_feature[i]), xy=(cv_range[i],val_accuracy[i]))\n",
    "plt.legend()\n",
    "plt.title('Cross Validation Accuracy')\n",
    "plt.xlabel('k fold')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n",
    "\n",
    "print('Best Accuracy with feature scaling and RFECV:', bestAcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('training_x Shape:', (1000, 40), ',training_y Shape:', (1000,), ',testing_x Shape:', (9000, 40))\n('x_all shape :', (10000, 40))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('best estimator KNN:', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n           weights='uniform'), 'Best Score', 0.996)\nFitting 10 folds for each of 1 candidates, totalling 10 fits\n[CV]  ................................................................\n[CV] ...................................... , score=1.0, total=   0.0s\n[CV]  ................................................................\n[CV] ...................................... , score=1.0, total=   0.0s\n[CV]  ................................................................\n[CV] ...................................... , score=1.0, total=   0.0s\n[CV]  ................................................................\n[CV] ..................................... , score=0.99, total=   0.0s\n[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ...................................... , score=1.0, total=   0.1s\n[CV]  ................................................................\n[CV] ...................................... , score=1.0, total=   0.1s\n[CV]  ................................................................\n[CV] ..................................... , score=0.99, total=   0.0s\n[CV]  ................................................................\n[CV] ...................................... , score=1.0, total=   0.0s\n[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ..................................... , score=0.98, total=   0.0s\n[CV]  ................................................................\n[CV] ...................................... , score=1.0, total=   0.0s\n('best estimator RandomForest:', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=None, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n            oob_score=False, random_state=None, verbose=0,\n            warm_start=False), 'Best Score', 0.998)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 0 0 0 0 1 0 0]\n[1 0 1 0 0 0 0 1 0 0]\n('Score for KNN :', 0.9960000000000001)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Score for Random Forest :', 1.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 0 0 0 0 1 0 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Id  Solution\n1        1         1\n2        2         0\n3        3         1\n4        4         0\n5        5         0\n6        6         0\n7        7         0\n8        8         1\n9        9         0\n10      10         0\n11      11         1\n12      12         1\n13      13         0\n14      14         0\n15      15         0\n16      16         1\n17      17         0\n18      18         0\n19      19         1\n20      20         1\n21      21         0\n22      22         1\n23      23         1\n24      24         0\n25      25         1\n26      26         1\n27      27         0\n28      28         0\n29      29         1\n30      30         0\n...    ...       ...\n8971  8971         1\n8972  8972         0\n8973  8973         1\n8974  8974         0\n8975  8975         0\n8976  8976         1\n8977  8977         0\n8978  8978         1\n8979  8979         1\n8980  8980         1\n8981  8981         1\n8982  8982         1\n8983  8983         0\n8984  8984         0\n8985  8985         0\n8986  8986         1\n8987  8987         0\n8988  8988         1\n8989  8989         1\n8990  8990         1\n8991  8991         0\n8992  8992         1\n8993  8993         1\n8994  8994         1\n8995  8995         1\n8996  8996         1\n8997  8997         1\n8998  8998         1\n8999  8999         0\n9000  9000         1\n\n[9000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#import sklearn as sk\n",
    "#import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.linear_model import Perceptron\n",
    "#from sklearn import tree\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "#from sklearn import svm\n",
    "\n",
    "#### READING OUR GIVEN DATA INTO PANDAS DATAFRAME ####\n",
    "x_train = train\n",
    "y_train = trainLabel\n",
    "x_test = test\n",
    "x_train = np.asarray(x_train)\n",
    "y_train = np.asarray(y_train)\n",
    "x_test = np.asarray(x_test)\n",
    "y_train = y_train.ravel()\n",
    "print('training_x Shape:',x_train.shape,',training_y Shape:',y_train.shape, ',testing_x Shape:',x_test.shape)\n",
    "\n",
    "#Checking the models\n",
    "x_all = np.r_[x_train,x_test]\n",
    "print('x_all shape :',x_all.shape)\n",
    "\n",
    "#### USING THE GAUSSIAN MIXTURE MODEL ####\n",
    "from sklearn.mixture import GaussianMixture\n",
    "lowest_bic = np.infty\n",
    "bic = []\n",
    "n_components_range = range(1, 7)\n",
    "cv_types = ['spherical', 'tied', 'diag', 'full']\n",
    "for cv_type in cv_types:\n",
    "    for n_components in n_components_range:\n",
    "        # Fit a mixture of Gaussians with EM\n",
    "        gmm = GaussianMixture(n_components=n_components,covariance_type=cv_type)\n",
    "        gmm.fit(x_all)\n",
    "        bic.append(gmm.aic(x_all))\n",
    "        if bic[-1] < lowest_bic:\n",
    "            lowest_bic = bic[-1]\n",
    "            best_gmm = gmm\n",
    "            \n",
    "best_gmm.fit(x_all)\n",
    "x_train = best_gmm.predict_proba(x_train)\n",
    "x_test = best_gmm.predict_proba(x_test)\n",
    "\n",
    "\n",
    "#### TAKING ONLY TWO MODELS FOR KEEPING IT SIMPLE ####\n",
    "knn = KNeighborsClassifier()\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "param_grid = dict( )\n",
    "#### GRID SEARCH for BEST TUNING PARAMETERS FOR KNN #####\n",
    "grid_search_knn = GridSearchCV(knn,param_grid=param_grid,cv=10,scoring='accuracy').fit(x_train,y_train)\n",
    "print('best estimator KNN:',grid_search_knn.best_estimator_,'Best Score', grid_search_knn.best_estimator_.score(x_train,y_train))\n",
    "knn_best = grid_search_knn.best_estimator_\n",
    "\n",
    "#### GRID SEARCH for BEST TUNING PARAMETERS FOR RandomForest #####\n",
    "grid_search_rf = GridSearchCV(rf, param_grid=dict( ), verbose=3,scoring='accuracy',cv=10).fit(x_train,y_train)\n",
    "print('best estimator RandomForest:',grid_search_rf.best_estimator_,'Best Score', grid_search_rf.best_estimator_.score(x_train,y_train))\n",
    "rf_best = grid_search_rf.best_estimator_\n",
    "\n",
    "\n",
    "knn_best.fit(x_train,y_train)\n",
    "print(knn_best.predict(x_test)[0:10])\n",
    "rf_best.fit(x_train,y_train)\n",
    "print(rf_best.predict(x_test)[0:10])\n",
    "\n",
    "#### SCORING THE MODELS ####\n",
    "print('Score for KNN :',cross_val_score(knn_best,x_train,y_train,cv=10,scoring='accuracy').mean())\n",
    "print('Score for Random Forest :',cross_val_score(rf_best,x_train,y_train,cv=10,scoring='accuracy').max())\n",
    "\n",
    "### IN CASE WE WERE USING MORE THAN ONE CLASSIFIERS THEN VOTING CLASSIFIER CAN BE USEFUL ###\n",
    "# clf = VotingClassifier(\n",
    "# \t\testimators=[('knn_best',knn_best),('rf_best',rf_best)], \n",
    "#         weights=[871856020222,0.907895269918]\n",
    "# \t)\n",
    "# clf.fit(x_train,y_train)\n",
    "# print clf.predict(x_test)[0:10]\n",
    "\n",
    "##### FRAMING OUR SOLUTION #####\n",
    "knn_best_pred = pd.DataFrame(knn_best.predict(x_test))\n",
    "rf_best_pred = pd.DataFrame(rf_best.predict(x_test))\n",
    "voting_clf_pred = pd.DataFrame(clf.predict(x_test))\n",
    "\n",
    "knn_best_pred.index += 1\n",
    "rf_best_pred.index += 1\n",
    "voting_clf_pred.index += 1\n",
    "\n",
    "rf_best_pred.columns = ['Solution']\n",
    "rf_best_pred['Id'] = np.arange(1,rf_best_pred.shape[0]+1)\n",
    "rf_best_pred = rf_best_pred[['Id', 'Solution']]\n",
    "print(rf_best_pred)\n",
    "\n",
    "#knn_best_pred.to_csv('knn_best_pred.csv')\n",
    "rf_best_pred.to_csv('./kaggle/data-science-london-scikit-learn/Submission_rf.csv', index=False)\n",
    "#voting_clf_pred.to_csv('voting_clf_pred.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
